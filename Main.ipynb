{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3261bd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as funcs\n",
    "from pyspark.sql.functions import col, when, round, rank, mean\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import IntegerType\n",
    "import mysql.connector\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6f6f6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_user = 'root'\n",
    "my_password = 'Hamed_11351'\n",
    "my_database='lahman2016'\n",
    "\n",
    "def connect_to_db():\n",
    "    return mysql.connector.connect(user=my_user, password=my_password, database=my_database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccdd6df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b4d05fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"lahman2016\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a78be350",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-D1DHCVG:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>lahman2016</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x20e2b09a880>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec48926",
   "metadata": {},
   "source": [
    "## Average Salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9a7546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to db\n",
    "db = connect_to_db()\n",
    "\n",
    "# Extract Data\n",
    "salaries_data = pd.read_sql_query(\n",
    "    '''select s.playerID, salary, f.yearID, POS\n",
    "    from salaries as s join fielding as f \n",
    "    on s.playerId=f.playerId and f.yearId=s.yearId;''', db) \n",
    "\n",
    "salaries_sc = spark.createDataFrame(salaries_data)\n",
    "\n",
    "# transform\n",
    "salaries_sc_2 = salaries_sc.groupby('yearID').pivot('POS').avg('salary')\n",
    "salaries_sc_3 = salaries_sc_2.withColumn('infield', (salaries_sc_2['1B'] + salaries_sc_2['2B'] + \\\n",
    "                                                     salaries_sc_2['3B'] + salaries_sc_2['SS'])/4)\n",
    "salaries_transformed = salaries_sc_3.select(col('yearID').alias('Year'), \n",
    "                                            round(col('infield'),0).cast(IntegerType()).alias('Fielding'),  \n",
    "                                            round(col('P'),0).cast(IntegerType()).alias('Pitching')).orderBy('yearID')\n",
    "\n",
    "# load\n",
    "salaries_transformed_pd = salaries_transformed.toPandas()\n",
    "salaries_transformed_pd['Pitching'] = salaries_transformed_pd.apply(lambda x: '{0:,}'.format(x['Pitching']), axis=1)\n",
    "salaries_transformed_pd['Fielding'] = salaries_transformed_pd.apply(lambda x: '{0:,}'.format(x['Fielding']), axis=1)\n",
    "salaries_transformed_pd.to_csv('Results\\AverageSalary.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf80821",
   "metadata": {},
   "source": [
    "## Hall of Fame All Star Pitchers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6e5b54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = connect_to_db()\n",
    "\n",
    "# extract   \n",
    "hof_data = pd.read_sql_query(\n",
    "    '''select playerID, yearid \n",
    "    from halloffame \n",
    "    where inducted=\"Y\"''', db)  \n",
    "\n",
    "pitchers_data = pd.read_sql_query(\n",
    "    '''select playerID, ERA\n",
    "    from pitching''', db) \n",
    "\n",
    "allstar_data = pd.read_sql_query(\n",
    "    '''select playerID, yearID as allstar_years, gameNum\n",
    "    from allstarfull \n",
    "    where GP=1''', db) \n",
    "\n",
    "# transform\n",
    "hof_sc = spark.createDataFrame(hof_data).alias('hof_sc')\n",
    "pitchers_sc = spark.createDataFrame(pitchers_data).alias('pitchers_sc')\n",
    "allstar_sc = spark.createDataFrame(allstar_data).alias('allstar_sc')\n",
    "\n",
    "pitchers_sc = pitchers_sc.groupby('playerID').avg('ERA')\\\n",
    "    .select(col('playerID'), round(col('avg(ERA)'),2).alias('ERA')).alias('pitchers_sc')\n",
    "\n",
    "hof_pitchers = pitchers_sc.join(hof_sc, hof_sc.playerID==pitchers_sc.playerID, how='inner')\\\n",
    "    .select(col('pitchers_sc.playerID'), col('yearid'), col('ERA')).alias('hof_pitchers')\n",
    "\n",
    "allstar_agg_sc = allstar_sc.withColumn('gameNum', when(col('gameNum')==0, 1).otherwise(col('gameNum')))\\\n",
    "    .groupby('playerID').sum('gameNum').select(col('playerID'), col('sum(gameNum)').alias('total_games'))\\\n",
    "    .alias('allstar_pitcher_num_games')\n",
    "\n",
    "hof_allstar_pitchers_transformed = hof_pitchers.join(allstar_agg_sc, allstar_agg_sc.playerID==hof_pitchers.playerID,how='inner')\\\n",
    "    .select(col('hof_pitchers.playerID').alias('Player'), col('ERA'), col('total_games').alias(' # All Star Appearances'),\n",
    "    col('yearid').alias('Hall of Fame Induction Year'))\n",
    "        \n",
    "# load\n",
    "hof_allstar_pitchers_transformed.toPandas().to_csv('HallofFameAllStarPitchers.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c3340f",
   "metadata": {},
   "source": [
    "## Pitching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8cff977a",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = connect_to_db()\n",
    "\n",
    "#extract\n",
    "post_players = pd.read_sql_query(\n",
    "    '''select  playerID, yearID, ER, IPouts, W, L, ERA\n",
    "    from pitchingpost''', db)     \n",
    "reg_players = pd.read_sql_query(\n",
    "    '''select  playerID, yearID, ER, IPouts, W, L, ERA \n",
    "    from pitching''', db)   \n",
    "\n",
    "#transform\n",
    "post_players_sc = spark.createDataFrame(post_players)\n",
    "reg_players_sc = spark.createDataFrame(reg_players)\n",
    "\n",
    "post_players_sc.createOrReplaceTempView(\"post_players_sc\")\n",
    "spark.sql('''\n",
    "    select playerID, yearID, avg(ERA) as reg_ERA, avg(ER) as ER, sum(IPouts) as IPouts, sum(W) as W, sum(L) as L, sum(W) + sum(L) as G\n",
    "    from post_players_sc\n",
    "    group by playerID, yearID''').createOrReplaceTempView(\"post_players_sc\")\n",
    "\n",
    "reg_players_sc.createOrReplaceTempView(\"reg_players_sc\")\n",
    "spark.sql('''\n",
    "    select playerID, yearID, avg(ERA) as post_ERA, avg(ER) as ER, sum(IPouts) as IPouts, sum(W) as W, sum(L) as L, sum(W) + sum(L) as G\n",
    "    from reg_players_sc\n",
    "    group by playerID, yearID''').createOrReplaceTempView(\"reg_players_sc\")\n",
    "\n",
    "spark.sql('''\n",
    "    select reg.yearID, reg.playerID, (reg.ER+post.ER) as ER, (reg.IPouts+post.IPouts)/3 as inningsPitched,\n",
    "        reg.W as reg_W, reg.L as reg_L, post.W as post_W, post.L as post_L, reg_ERA, post_ERA\n",
    "    from post_players_sc as post join reg_players_sc as reg\n",
    "    on post.playerID=reg.playerID and post.yearID=reg.yearID\n",
    "    where post.G>0''').createOrReplaceTempView(\"all_players_sc\")\n",
    "\n",
    "spark.sql('''\n",
    "    select * from (select yearID, PLAYERID, rank() over(partition by yearID order by ER/inningsPitched) as Rank, \n",
    "        reg_W/(reg_W+reg_L) as RegSeasonWinLoss, post_W/(post_W+post_L) as PostSeasonWinLoss, reg_ERA, post_ERA\n",
    "    from all_players_sc) as a \n",
    "    where Rank<=10''').createOrReplaceTempView('best_pitchers_per_year')\n",
    "\n",
    "pitching_transformed = spark.sql('''\n",
    "    select yearID, playerID, round(reg_ERA,2) as RegualarERA, round(RegSeasonWinLoss,2) as RegSeasonWinLoss,\n",
    "        round(post_ERA,2) as PostERA, round(PostSeasonWinLoss,2) as PostSeasonWinLoss\n",
    "    from best_pitchers_per_year order by 2 desc''')\n",
    "\n",
    "pitching_transformed = pitching_transformed.select(col('yearID').alias('Year'), \n",
    "                                                   col('playerID').alias('Player'), \n",
    "                                                   col('RegualarERA').alias('Regular Season ERA'), \n",
    "                                                   col('RegSeasonWinLoss').alias('Regular Season Win/Loss'), \n",
    "                                                   col('PostERA').alias('Post-season ERA'), \n",
    "                                                   col('PostSeasonWinLoss').alias('Post-season Win/Loss')).orderBy('yearID')\n",
    "\n",
    "#load\n",
    "pitching_transformed.toPandas().to_csv('Pitching.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90f1d1d",
   "metadata": {},
   "source": [
    "## Rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11c09add",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = connect_to_db()\n",
    "\n",
    "#extract\n",
    "team_data = pd.read_sql_query(\n",
    "    '''select yearID, teamID, W, L, AB from teams''', db)  \n",
    "\n",
    "team_sc = spark.createDataFrame(team_data).alias('team_sc')\n",
    "\n",
    "        \n",
    "#transform\n",
    "team_sc_wl = team_sc.withColumn('W/L', (col('W')/col('L')))\n",
    "windowSpec  = Window.partitionBy('yearID').orderBy('W/L')\n",
    "rank_sc = team_sc_wl.withColumn('rank',rank().over(windowSpec))\n",
    "best_team = rank_sc.filter(col('rank')==1).select('yearID', 'rank', 'teamID', 'AB')\n",
    "worst_rank_years = rank_sc.groupby('yearID').max('rank').select(col('yearID'),col('max(rank)').alias('rank'))\n",
    "worst_team = rank_sc.join(worst_rank_years, [worst_rank_years.yearID==rank_sc.yearID,\\\n",
    "                                             worst_rank_years.rank==rank_sc.rank], how='inner' )\n",
    "worst_team = spark.createDataFrame(worst_team.toPandas().iloc[:,[0, 6, 1, 4]])\n",
    "rankings_transformed = best_team.union(worst_team).orderBy('yearID').\\\n",
    "    select(col('teamID').alias('Team ID'), col('yearID').alias('Year'), col('rank').alias('Rank') ,col('AB').alias('At Bats'))\n",
    "\n",
    "#load\n",
    "rankings_transformed.toPandas().to_csv('Rankings.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
